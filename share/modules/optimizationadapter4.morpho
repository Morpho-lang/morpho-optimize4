/* ************************************************************
 * OptimizationAdapter 
 * ************************************************************ */ 

/* --------------------------
 * Errors
 * -------------------------- */ 

var _OptConsCountErr = Error("OptConsCount", "More equality constraints specified than provided.")

/* --------------------------
 * OptimizationAdapter class
 * -------------------------- */ 

/* OptimizationAdapters provide a common interface that enables optimization algorithms to obtain 
   necessary information about the an optimization problem. */

class OptimizationAdapter {
  init(target) { self.target = target }

  set(x) { return nil }               // Sets the parameters 
  get() { return 0 }                  // Returns current value of the parameters

  value() { return 0 }                // Returns the current value of the objective function at the current parameters
  gradient() { return nil }           // Returns the gradient of the objective function at the current parameters as a column vector
  hessian() { return nil }            // (Optional) Returns the hessian of the objective function at the current parameters as a matrix

  countConstraints() { return 0 }     // Returns number of constraints present
  countEqualityConstraints() { return 0 } // Returns number of equality constraints present
  countInequalityConstraints() {      // Returns number of inequality constraints present
    return self.countConstraints() - self.countEqualityConstraints() 
  } 

  constraintValue() { return nil }    // Returns the value(s) of any constraints as a list of numbers and vectors
  constraintGradient() { return nil } // Returns the gradient(s) of any constraints as a list of vectors or matrices
  constraintHessian() { return nil }  // Returns the hessian(s) of any constraints as a list of matrices, or sublists
}

/* --------------------------
 * DelegateAdapter class
 * -------------------------- */ 

/** A delegate adapter fully implements the adapter protocol, but instead passes
    method calls on to a second adapter. Useful as a base class for adapters 
    that are intended to work with other adapters */

class DelegateAdapter is OptimizationAdapter {
  init(OptimizationAdapter adapter) {
    self.adapter = adapter
  }

  set(x) { self.adapter.set(x) }
  get() { return self.adapter.get() }

  value() { return self.adapter.value() }
  gradient() { return self.adapter.gradient() }
  hessian() { return self.adapter.hessian() }

  countConstraints() { return self.adapter.countConstraints() }
  countEqualityConstraints() { return self.adapter.countEqualityConstraints() }
  countInequalityConstraints() { return self.adapter.countInequalityConstraints() }

  constraintValue() { return self.adapter.constraintValue() }
  constraintGradient() { return self.adapter.constraintGradient() }
  constraintHessian() { return self.adapter.constraintHessian() } 
}


/* -------------------------
 * FunctionAdapter 
 * ------------------------- */

/* Adapts a provided function (and an optional gradient and hessian) 
   to the OptimizationAdapter interface */

var _epsfd=(1e-16)^(1/3) // Optimal relative stepsize for cell centered gradient
var _epshess=(1e-16)^(1/4) // Optimal relative stepsize for cell centered hessian

class FunctionAdapter is OptimizationAdapter {
  init(target, gradient=nil, hessian=nil, start=nil, 
               constraints=nil, constraintgradients=nil, constrainthessians=nil,
               equalitycount=nil) {
    super.init(target)
    self.x = start
    
    self.gradfn = gradient 
    if (!gradient) self.gradfn = self._numericalgrad(self.target) // Use FD if no grad provided

    self.hessianfn = hessian
    if (!hessian) self.hessianfn = self._numericalhessian(self.target) // Use FD if no hessian provided

    self.constraints = constraints

    self.constraintcount = 0 
    if (constraints) self.constraintcount = self.constraints.count()
    self.equalitycount = self.constraintcount // By default, all constraints are equality constraints
    if (equalitycount) self.equalitycount = equalitycount

    if (self.equalitycount>self.constraintcount) _OptConsCountErr.throw()

    self.constraintgradfn = constraintgradients
    if (self.constraints && !self.constraintgradfn) {
      var lst = []
      for (c in self.constraints) lst.append(self._numericalgrad(c))
      self.constraintgradfn = lst 
    }

    self.constrainthessianfn = constrainthessians
    if (self.constraints && !self.constrainthessianfn) {
      var lst = []
      for (c in self.constraints) lst.append(self._numericalhessian(c))
      self.constrainthessianfn = lst 
    }
  }

  _listify(x) { // Converts an enumerable to a list
    var l = []
    for (e in x) l.append(e)
    return l 
  }

  _stepsize(eps, x) { // Select stepsize for finite differences
    var h=eps
    var xx=abs(x)
    if (xx>1) h*=xx // Scale

    var temp = x+h  // Ensure we obtain an FP representable number
    return temp - x 
  }

  _numericalgrad(f) { // Construct a numerical gradient as a closure 
    fn numericalgradfn(...x) {
      var n = x.count()
      var grad = Matrix(n)

      for (var i=0; i<n; i+=1) {
        var x0 = x[i]
        var eps = self._stepsize(_epsfd, x0)
        x[i]=x0+eps
        var fr = apply(f, x)
        x[i]=x0-eps
        var fl = apply(f, x)
        grad[i]=(fr-fl)/(2*eps)
      }

      return grad 
    }
    return numericalgradfn
  }

  _numericalhessian(f) {
    fn numericalhessianfn(...x) {
      var n = x.count()
      var hess = Matrix(n,n)
      var f0 = apply(f, x)

      for (var i=0; i<n; i+=1) {
        var x0 = x[i]
        var epsx = self._stepsize(_epsfd, x0)

        // Diagonal entries see Abramowitz and Stegun 1972, p. 884, 25.3.23, O(h^2)
        x[i]=x0+epsx
        var fr = apply(f, x)
        x[i]=x0-epsx
        var fl = apply(f, x)
        x[i]=x0

        hess[i,i]=(fr+fl-2*f0)/(epsx*epsx)

        // Off diagonal entries Abramowitz and Stegun 1972, p. 884, 25.3.26 O(h^2)
        for (var j=0; j<i; j+=1) {
          var y0 = x[j]            
          var epsy = self._stepsize(_epsfd, y0)

          x[i]=x0+epsx; x[j]=y0+epsy
          var frr = apply(f, x)
          x[i]=x0-epsx; x[j]=y0-epsy
          var fll = apply(f, x)
          x[i]=x0-epsx; x[j]=y0+epsy
          var flr = apply(f, x)
          x[i]=x0+epsx; x[j]=y0-epsy
          var frl = apply(f, x)
          x[i]=x0; x[j]=y0

          hess[i,j]=hess[j,i]=(frr+fll-flr-frl)/(epsx*epsy)/4
        }
      }

      return hess  
    } 
    return numericalhessianfn 
  }

  set(x) { self.x = x }
  get() { return self.x }

  value() { return apply(self.target, self._listify(self.x)) }
  gradient() { return apply(self.gradfn, self._listify(self.x)) }
  hessian() { return apply(self.hessianfn, self._listify(self.x)) }

  countConstraints() { return self.constraintcount }
  countEqualityConstraints() { return self.equalitycount }

  constraintMap(list) {
    var val = []
    for (c, k in list) val.append( apply(c, self._listify(self.x)) )
    return val 
  } 

  constraintValue() { 
    if (!self.constraints) return nil 
    return self.constraintMap(self.constraints)
  }   

  constraintGradient() { 
    if (!self.constraints) return nil 
    return self.constraintMap(self.constraintgradfn)
  }

  constraintHessian() { 
    if (!self.constraints) return nil 

    if (!self.constraints) return nil 
    return self.constraintMap(self.constrainthessianfn)
  }
}

