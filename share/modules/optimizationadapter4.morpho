/* ************************************************************
 * OptimizationAdapter 
 * ************************************************************ */ 

/* --------------------------
 * Errors
 * -------------------------- */ 

var _OptConsCountErr = Error("OptConsCount", "More equality constraints specified than provided.")
var _OptUnConsErr = Error("OpUncons", "Problem is unconstrained; this adapter is intended for constrained problems.")
var _OptNoHessErr = Error("OptNoHess", "Method requires a hessian.")
var _OptHessErr = Error("OptHess", "Adapter cannot be used with methods that require a hessian.")

var _OptNoActiveFuncErr = Error("OptNoActiveFunc", "Problem has no active functionals.")
var _OptTrgtLstEmptyErr = Error("OptTrgtLstEmpty", "List of optimization targets is empty.")

var _OptNoPrblmErr = Error("OptNoPrblm", "ProblemAdapter must be initialized with an OptimizationProblem.")
var _OptPrblmTrgtErr = Error("OptPrblmTrgt", "ProblemAdapter must be initialized with Meshes or Fields as targets.")

var _OptEvlFled = Error("OptFnctnlEvlFld", "Evaluation of functional failed.")

var _OptNonDiffErr = Error("OptNonDiff", "Objective function is nondifferentiable; use a Controller that avoids differentiation.")

/* -------------------------
 * Utility Functions
 * ------------------------- */

fn _columnize(matrix) { // Reshape a rectangular matrix into a column matrix 
  var dim = matrix.dimensions() 
  matrix.reshape(dim[0]*dim[1],1)
}

fn _decolumnize(matrix, dim) { // Reshape a matrix from a column matrix to a rectangular matrix
  matrix.reshape(dim[0], dim[1])
}

/* --------------------------
 * OptimizationAdapter class
 * -------------------------- */ 

/** OptimizationAdapters provide a common interface that enables optimization algorithms to obtain 
    necessary information about the an optimization problem. */

class OptimizationAdapter {
  init(target) { self.target = target }

  set(x) { return nil }               // Sets the parameters 
  get() { return 0 }                  // Returns current value of the parameters

  value() { return 0 }                // Returns the current value of the objective function at the current parameters
  gradient() { return nil }           // Returns the gradient of the objective function at the current parameters as a column vector
  hessian() { return nil }            // (Optional) Returns the hessian of the objective function at the current parameters as a matrix

  countConstraints() { return 0 }     // Returns number of constraints present
  countEqualityConstraints() { return 0 } // Returns number of equality constraints present
  countInequalityConstraints() {      // Returns number of inequality constraints present
    return self.countConstraints() - self.countEqualityConstraints() 
  } 

  constraintValue() { return nil }    // Returns the value(s) of any constraints as a list of numbers and vectors
  constraintGradient() { return nil } // Returns the gradient(s) of any constraints as a list of vectors or matrices
  constraintHessian() { return nil }  // Returns the hessian(s) of any constraints as a list of matrices, or sublists
}

/* --------------------------
 * DelegateAdapter class
 * -------------------------- */ 

/** A delegate adapter fully implements the adapter protocol, but instead passes
    method calls on to a second adapter. Useful as a base class for adapters 
    that are intended to work with other adapters */

class DelegateAdapter is OptimizationAdapter {
  init(adapter) {
    self.adapter = adapter
  }

  set(x) { self.adapter.set(x) }
  get() { return self.adapter.get() }

  value() { return self.adapter.value() }
  gradient() { return self.adapter.gradient() }
  hessian() { return self.adapter.hessian() }

  countConstraints() { return self.adapter.countConstraints() }
  countEqualityConstraints() { return self.adapter.countEqualityConstraints() }
  countInequalityConstraints() { return self.adapter.countInequalityConstraints() }

  constraintValue() { return self.adapter.constraintValue() }
  constraintGradient() { return self.adapter.constraintGradient() }
  constraintHessian() { return self.adapter.constraintHessian() } 
}

/* --------------------------
 * DeconstrainAdapter class
 * -------------------------- */ 

/** An DeconstrainAdapter converts a constrained problem to an unconstrained problem, 
    ignoring any constraints */

class DeconstrainAdapter is OptimizationAdapter {
  init(adapter) {
    self.adapter = adapter
  }

  set(x) { self.adapter.set(x) }
  get() { return self.adapter.get() }

  value() { return self.adapter.value() }
  gradient() { return self.adapter.gradient() }
  hessian() { return self.adapter.hessian() }
}

/* --------------------------
 * ProxyAdapter class
 * -------------------------- */ 

/** A ProxyAdapter calls the methods of a provided adapter, but caches values
    to prevent multiple evaluations. Can provide a report of the number of 
    evaluations of each quantity. */

class ProxyAdapter is DelegateAdapter {
  init(adapter) {
    super.init(adapter)
    self.reset() 
    self.count = [0,0,0,0,0,0]
  }

  reset() {
    self._value = nil 
    self._gradient = nil 
    self._hessian = nil 
    self._constraintValue = nil 
    self._constraintGradient = nil 
    self._constraintHessian = nil 
  }

  set(x) { 
    self.adapter.set(x) 
    self.reset() 
  }

  get() { return self.adapter.get() }

  _compute(prop, func, counter) {
    if (!self[prop]) { 
      self.setindex(prop, func())
      self.count[counter]+=1
    }
    return self[prop]
  }

  value()    { return self._compute("_value", self.adapter.value, 0) }
  gradient() { return self._compute("_gradient", self.adapter.gradient, 1) }
  hessian() { return self._compute("_hessian", self.adapter.hessian, 2) }

  directionalDerivative(d) { return self.adapter.directionalDerivative(d) }

  constraintValue() { return self._compute("_constraintValue", self.adapter.constraintValue, 3) }
  constraintGradient() { return self._compute("_constraintGradient", self.adapter.constraintGradient, 4) }
  constraintHessian() { return self._compute("_constraintHessian", self.adapter.constraintHessian, 4) }

  countEvals() { // Returns the evaluation count
    return self.count 
  }

  report() { // Reports the number of calls 
    var out = "" 
    var desc = ("Fn", "Grad", "Hess", "Cons.", "Cons. grad", "Cons. hess")

    for (count, k in self.count) {
      if (count>0) {
        if (k>0) out+=", "
        out+="${desc[k]} evals: ${count}"
      }
    }

    print out 
  }
}

/* -------------------------
 * FixAdapter 
 * ------------------------- */

/* Fixes selected variables in a problem, preventing them from being changed.
   Their gradients are also set to zero. */

class FixAdapter is DelegateAdapter {
  init(adapter, List fix) {
    super.init(adapter)
    self.fix = fix 
  }

  init(adapter, Int fix) {
    self.init(adapter, [fix])
  }

  set(x) { // Fixed variables are copied across
    var ox = self.get() 
    for (i in self.fix) x[i] = ox[i]
    self.adapter.set(x)
  }

  _zeroElements(g) {
    for (i in self.fix) g[i]=0
  }

  gradient() { // Set the gradient of any fixed variables to zero 
    var grad = self.adapter.gradient() 
    _zeroElements(grad)
    return grad 
  }

  constraintGradient() {
    var grad = self.adapter.constraintGradient() 
    for (c in grad) _zeroElements(c)
    return grad
  }

  hessian() { _OptHessErr.throw() }
  constraintHessian() { _OptHessErr.throw() } 
}

/* -------------------------
 * ConstraintVector
 * ------------------------- */

/** Mixin that provides constraintVector(), which converts the results of constraintValue() 
    into a single vector of values. */

class ConstraintVectorMixin {
  constraintVector() {
    var cv = self.adapter.constraintValue()

    if (!cv) _OptUnConsErr.warning()

    var v = self._flatten(cv)
    var neq = self._counteq(cv)
    self._checkineq(v, neq)

    return v 
  }

  _counteq(cv) { // Counts the number of equality constraints
    var nc = self.adapter.countEqualityConstraints()
    var neq = 0
    for (c, k in cv) {
      if (k>=nc) break
      if (ismatrix(c)) neq+=c.count()
      else neq+=1
    }  
    return neq
  }

  _flatten(v) {
    var l = []
    for (c in v) {
      if (ismatrix(c)) for (x in c) l.append(x)
      else l.append(c)
    }
    return Matrix(l)
  } 

  _checkineq(c, neq) {
    for (k in neq...c.count()) {
      if (c[k]>0) c[k]=0
    }
  }
}

/* -------------------------
 * PenaltyAdapter
 * ------------------------- */

/** Converts a constrained optimization problem into an unconstrained one 
    by including constraints 
       F = f + mu *|c|^2 + mu* |d-|^2 
    Used to implement the penalty method */

class PenaltyAdapter is OptimizationAdapter with ConstraintVectorMixin {
  init(adapter, penalty=1) { 
    self.adapter = adapter
    self.penalty = penalty 
  }

  setpenalty(penalty) { self.penalty = penalty }
  penalty() { return self.penalty }

  set(x) { self.adapter.set(x) }
  get() { return self.adapter.get() }

  value() { // Q = f + mu*c^2 + mu*d-^2
    var f = self.adapter.value() 
    var v = self.constraintVector() 
    if (v) f += self.penalty * v.inner(v)

    return f
  }

  gradient() { // grad Q = grad f + 2*mu*g*grad g
    var grad = self.adapter.gradient() 
    var v = self.constraintVector()   

    if (v) {
      var dg = self.adapter.constraintGradient()   
      var c = Matrix([dg])

      grad+=2*self.penalty*c*v  
    } 
  
    return grad
  }

  directionalDerivative(d) {
    var df = self.gradient() 
    return df.inner(d) 
  }

  hessian() { // hessian Q = hessian f + 2*mu*g*hessian g + 2*mu*(dg \outer dg)
    var hess = self.adapter.hessian() 
    if (!hess) _OptNoHessErr.throw() 

    var v = self.constraintVector() 
    
    if (v) {
      var dg = self.adapter.constraintGradient()  
      var hessg = self.adapter.constraintHessian() 

      for (vk, k in v) {
        if (abs(vk)==0) continue
        if (!hessg[k]) _OptNoHessErr.throw() 

        hess+=2*self.penalty*(vk*hessg[k] + dg[k].outer(dg[k]))
      }
    }

    return hess
  }
}

/* -------------------------
 * ReprojectionAdapter
 * ------------------------- */

/** Converts a constrained optimization problem into an unconstrained one 
    consisting purely of the 2-norm of the constraints. 
       F = |c|^2 + |d-|^2 
    Used to reproject a solution back onto the feasible set */

class ReprojectionAdapter is DeconstrainAdapter with ConstraintVectorMixin {
  value() { // F = |c|^2 + |d-|^2 
    var v = self.constraintVector() 
    return v.inner(v)
  }
  
  gradient() { // grad F = 2*g*grad g
    var v = self.constraintVector()   
    var dg = self.adapter.constraintGradient()   
    var c = Matrix([dg])

    return 2*c*v  
  }
}

/* -------------------------
 * L1PenaltyAdapter
 * ------------------------- */

/** Converts a constrained optimization problem into an unconstrained one 
    using an L1 norm: 
       F = f + mu*|c| + mu*|d-| 
    The resulting function is nondifferentiable, but does admit a 
    directional derivative. 
    Commonly used as a merit function. */

class L1PenaltyAdapter is PenaltyAdapter {
  value() { // F = f + mu*|c| + mu*|d-|
    var f = self.adapter.value() 
    var c = self.constraintVector()  

    if (c) f += self.penalty * c.norm(1)

    return f
  }

  gradient() { _OptNonDiffErr.throw() }
  hessian() { _OptNonDiffErr.throw() }

  directionalDerivative(d) {
    var df = self.adapter.gradient() 
    var c = self.constraintVector()

    return df.inner(d) - self.penalty*c.norm(1)
  }
}

/* -------------------------
 * LagrangeMultiplierAdapter
 * ------------------------- */

/** Converts a constrained optimization problem into an unconstrained one 
    using lagrange multipliers:
       F = f + lambda . c + lambda' . d-
    Used to implement SQP */

class LagrangeMultiplierAdapter is OptimizationAdapter {
  init(adapter) { 
    self.adapter = adapter
    self._initialLagrangeMultipliers() 
  }

  _initialLagrangeMultipliers() { // Initialize lagrange multipliers to 1
    var cv = self.adapter.constraintValue()
    self.lambda = []
    for (c in cv) self.lambda.append(1+0*c)
    self._checkLagrangeMultipliers()
  }

  _checkLagrangeMultipliers() { // Zeros lagrange multipliers for inactive constraints
    var cv = self.adapter.constraintValue() 
    self._checkIneq(cv, self.lambda)
  }

  set(x) { // Sets parameters, separating off the lagrange multipliers and storing them
    var n = x.count()
    var nvars = self.adapter.get().count()
    self.adapter.set(x[0...nvars,0])
    self.lambda = self._unflatten(x[nvars...n,0], self.lambda) 
    self._checkLagrangeMultipliers()
  }

  get() { // Get parameters, joining real parameters with lagrange multipliers
    var x = self.adapter.get()
    return Matrix([[x], [self._flatten(self.lambda)]]) 
  }

  _unflatten(Matrix v, shape) { // Converts a Vector into a list of scalars and vectors according to a given shape
    var l = []
    var n = 0 
    for (u, k in shape) {
      if (ismatrix(u)) {
        var m = u.count()
        l.append(v[n...n+m,0])
        n+=m
      } else {
        l.append(v[n])
        n+=1
      }
    }

    return l 
  }  

  _flatten(v) { // Flattens a list of scalars and vectors into a Vector
    var l = []
    for (c in v) {
      if (ismatrix(c)) for (x in c) l.append(x)
      else l.append(c)
    }
    return Matrix(l)
  } 

  setLagrangeMultipliers(lambda) {
    self.lambda = self._unflatten(lambda, self.lambda)
  }

  lagrangeMultipliers() {
    return self._flatten(self.lambda)
  }

  _zeroIneq(Matrix m, Matrix out) {
    for (c, k in m) {
      if (c>=0) out[k] = 0
    }
    return out 
  }

  _zeroIneq(u, out) {
    if (u>=0) return 0
    return out
  }

  _checkIneq(c, out) { // Checks whether entries in c are inside the feasible 
                       // region and sets the corresponding entry in out to zero
    var neq = self.adapter.countEqualityConstraints()
    for (k in neq...c.count()) {
      out[k]=self._zeroIneq(c[k], out[k])
    }
  }

  _inner(Matrix u, Matrix v) { return u.inner(v) }
  _inner(u, v) { return u*v }

  inner(c) { // Takes the inner product of the list c with the lagrange multipliers
    var total = 0 
    for (u,k in c) total+=self._inner(self.lambda[k], u)
    return total
  }

  value() { // Lagrangian function = f - lambda_i c_i
    var f = self.adapter.value() 
    var c = self.adapter.constraintValue()
    self._checkIneq(c, c)

    return f - self.inner(c)
  }

  varGradient() { // Gradient of lagrangian wrt original degrees of freedom
    var grad = self.adapter.gradient()
    var cgrad = self.adapter.constraintGradient()

    for (cg, k in cgrad) grad -= cg*self.lambda[k]

    return grad 
  }

  gradient() { // Gradient of Lagrangian is [ df - lambda_i dc_i , -c_i] 
    var c = self.adapter.constraintValue()
    self._checkIneq(c, c)
    var cv = self._flatten(c)

    return Matrix([[self.varGradient()],[-cv]]) 
  } 

  _checkConstraintValue(Matrix u, isEq, out) {
    for (c in u) {
      if (isEq || c<0) out.append(c)
    }
  }

  _checkConstraintValue(u, isEq, out) {
    if (isEq || u<0) out.append(u)
  }

  activeConstraintValue() {
    var cv = self.adapter.constraintValue()
    var neq = self.adapter.countEqualityConstraints()
    var l = []
    for (c, k in cv) {
      self._checkConstraintValue(c, k<neq, l)
    }
    return Matrix(l)
  }

  _checkConstraintGradient(Matrix u, grad, isEq, out) {
    if (isEq) {
      out.append(grad)
    } else {
      var l = []
      for (c, k in u) {
        if (isEq || c<0) l.append(grad.column(k))
      }
      out.append(Sparse([l]))
    }
  }

  _checkConstraintGradient(u, grad, isEq, out) {
    if (isEq || u<0) out.append(grad)
  }

  activeConstraintGradient() {
    var cv = self.adapter.constraintValue()
    var cgrad = self.adapter.constraintGradient()
    var neq = self.adapter.countEqualityConstraints()
    var l = []
    for (c, k in cv) {
      self._checkConstraintGradient(c, cgrad[k], k<neq, l)
    }
    return l 
  }

  activeUpdate(dl) { // Promote update onto active set
    var cv = self.adapter.constraintValue()
    var neq = self.adapter.countEqualityConstraints()
    var out = []
    var dli = 0 // Index into dl 
    for (c, k in cv) {
      if (ismatrix(c)) {
        for (u in c) {
          if (k<neq || u<0) {
            out.append(dl[dli])
            dli += 1 
          } else out.append(0)
        }
      } else {
        if (k<neq || c<0) {
          out.append(dl[dli])
          dli += 1 
        } else out.append(0)
      }
    }
    return Matrix(out)
  }
}

/* -------------------------
 * FunctionAdapter 
 * ------------------------- */

/* Adapts a provided function (and an optional gradient and hessian) 
   to the OptimizationAdapter interface */

var _epsfd=(1e-16)^(1/3) // Optimal relative stepsize for cell centered gradient
var _epshess=(1e-16)^(1/4) // Optimal relative stepsize for cell centered hessian

class FunctionAdapter is OptimizationAdapter {
  init(target, gradient=nil, hessian=nil, start=nil, 
               constraints=nil, constraintgradients=nil, constrainthessians=nil,
               equalitycount=nil) {
    super.init(target)
    self.x = start
    
    self.gradfn = gradient 
    if (!gradient) self.gradfn = self._numericalgrad(self.target) // Use FD if no grad provided

    self.hessianfn = hessian
    if (!hessian) self.hessianfn = self._numericalhessian(self.target) // Use FD if no hessian provided

    self.constraints = constraints

    self.constraintcount = 0 
    if (constraints) self.constraintcount = self.constraints.count()
    self.equalitycount = self.constraintcount // By default, all constraints are equality constraints
    if (equalitycount) self.equalitycount = equalitycount

    if (self.equalitycount>self.constraintcount) _OptConsCountErr.throw()

    self.constraintgradfn = constraintgradients
    if (self.constraints && !self.constraintgradfn) {
      var lst = []
      for (c in self.constraints) lst.append(self._numericalgrad(c))
      self.constraintgradfn = lst 
    }

    self.constrainthessianfn = constrainthessians
    if (self.constraints && !self.constrainthessianfn) {
      var lst = []
      for (c in self.constraints) lst.append(self._numericalhessian(c))
      self.constrainthessianfn = lst 
    }
  }

  _listify(x) { // Converts an enumerable to a list
    var l = []
    for (e in x) l.append(e)
    return l 
  }

  _stepsize(eps, x) { // Select stepsize for finite differences
    var h=eps
    var xx=abs(x)
    if (xx>1) h*=xx // Scale

    var temp = x+h  // Ensure we obtain an FP representable number
    return temp - x 
  }

  _numericalgrad(f) { // Construct a numerical gradient as a closure 
    fn numericalgradfn(...x) {
      var n = x.count()
      var grad = Matrix(n)

      for (var i=0; i<n; i+=1) {
        var x0 = x[i]
        var eps = self._stepsize(_epsfd, x0)
        x[i]=x0+eps
        var fr = apply(f, x)
        x[i]=x0-eps
        var fl = apply(f, x)
        grad[i]=(fr-fl)/(2*eps)
      }

      return grad 
    }
    return numericalgradfn
  }

  _numericalhessian(f) {
    fn numericalhessianfn(...x) {
      var n = x.count()
      var hess = Matrix(n,n)
      var f0 = apply(f, x)

      for (var i=0; i<n; i+=1) {
        var x0 = x[i]
        var epsx = self._stepsize(_epshess, x0)

        // Diagonal entries see Abramowitz and Stegun 1972, p. 884, 25.3.23, O(h^2)
        x[i]=x0+epsx
        var fr = apply(f, x)
        x[i]=x0-epsx
        var fl = apply(f, x)
        x[i]=x0

        hess[i,i]=(fr+fl-2*f0)/(epsx*epsx)

        // Off diagonal entries Abramowitz and Stegun 1972, p. 884, 25.3.26 O(h^2)
        for (var j=0; j<i; j+=1) {
          var y0 = x[j]            
          var epsy = self._stepsize(_epshess, y0)

          x[i]=x0+epsx; x[j]=y0+epsy
          var frr = apply(f, x)
          x[i]=x0-epsx; x[j]=y0-epsy
          var fll = apply(f, x)
          x[i]=x0-epsx; x[j]=y0+epsy
          var flr = apply(f, x)
          x[i]=x0+epsx; x[j]=y0-epsy
          var frl = apply(f, x)
          x[i]=x0; x[j]=y0

          hess[i,j]=hess[j,i]=(frr+fll-flr-frl)/(epsx*epsy)/4
        }
      }

      return hess  
    } 
    return numericalhessianfn 
  }

  set(x) { self.x = x }
  get() { return self.x }

  value() { return apply(self.target, self._listify(self.x)) }
  gradient() { return apply(self.gradfn, self._listify(self.x)) }
  hessian() { return apply(self.hessianfn, self._listify(self.x)) }

  countConstraints() { return self.constraintcount }
  countEqualityConstraints() { return self.equalitycount }

  constraintMap(list) {
    var val = []
    for (c, k in list) val.append( apply(c, self._listify(self.x)) )
    return val 
  } 

  constraintValue() { 
    if (!self.constraints) return nil 
    return self.constraintMap(self.constraints)
  }   

  constraintGradient() { 
    if (!self.constraints) return nil 
    return self.constraintMap(self.constraintgradfn)
  }

  constraintHessian() { 
    if (!self.constraints) return nil 

    if (!self.constraints) return nil 
    return self.constraintMap(self.constrainthessianfn)
  }
}

/* -------------------------
 * FixSelectionMixin
 * ------------------------- */

/** Mixin used to help identify indices to fix from a selection */

class FixSelectionMixin {
  _indexOffsets() { // Construct a dictionary of offsets for each target
    var offsetDict = Dictionary() 
    Int ix = 0
    for (t in self.target) {
      offsetDict[t]=ix
      ix+=self.count(t)
    }
    return offsetDict
  }

  _dimension(Mesh m) { return m.vertexmatrix().dimensions()[0] }
  _dimension(Field f) { return f.prototype().count() }

  _idToIndex(Mesh target, Int id) { return _dimension(target)*id }
  _idToIndex(Field target, Int id) { Error("NtImplmntd", "Indexing field elements not implemented").throw() }

  selectionToIndexList(Selection sel, target, components=nil) {
    var offsetDict = _indexOffsets()
    var ixoffset = offsetDict[target]

    var dim = _dimension(target)

    var list = []
    for (id in sel.idlistforgrade(0)) {
      var ix = _idToIndex(target, id)
      for (i in 0...dim) {
        if (isobject(components) && !components.contains(i)) continue // Skip anything not in the components list 
        list.append(ixoffset+ix+i)
      }
    }
    return list
  }
}

/* -------------------------
 * ProblemAdapter
 * ------------------------- */

class ProblemAdapter is OptimizationAdapter with FixSelectionMixin {
  init(OptimizationProblem problem, ...target, mesh=nil) {
    self.problem = problem 
    self.target = [] // Keep track of optimization targets 
    for (t in target) {
      if (ismesh(t) || isfield(t)) self.target.append(t)
      else _OptPrblmTrgtErr.throw() 
    }

    // Basic validation
    if (self.target.count()==0) _OptTrgtLstEmptyErr.throw()
    if (!islist(self.problem.energies) || self.problem.energies.count()==0) _OptNoActiveFuncErr.throw()
  }

  init(target) { _OptNoPrblmErr.throw() }

  get() { // Returns current value of parameters
    if (self.target.count()==1) return self.columnVector(self.target[0])

    var l = []
    for (t in self.target) l.append([self.columnVector(t)])
    return Matrix(l)
  }                  

  columnVector(Mesh m) { // Extracts the positions from a Mesh as a column vector
    var v = m.vertexmatrix().clone()
    _columnize(v) 
    return v 
  }

  columnVector(Field f) { // Extracts the contents of a Mesh as a column vector
    return f.linearize() 
  }

  columnVector(Matrix m) { // Converts a matrix to a column vector
    _columnize(m) 
    return m
  }

  set(x) { // Sets the parameters 
    if (self.target.count()==1) return self.set(self.target[0], x)

    var n = 0
    for (t in self.target) {
      var ndof = self.count(t)
      self.set(t, x[n...n+ndof]) 
      n+=ndof
    }
    return nil 
  }              

  set(Mesh m, Matrix x) { 
    var mat = m.vertexmatrix() // Extract the vertex matrix and edit it directly
    _decolumnize(x, mat.dimensions())
    mat.assign(x)
    _columnize(x) // Restore x to column vector form
    return nil  
  }

  set(Field f, Matrix x) {   
    f.__linearize().assign(x)
    return nil
  }

  count() { // Count the total number of degrees of freedom in the problem
    var ndof = 0
    for (t in self.target) ndof += self.count(t)
    return ndof 
  }

  count(Mesh m) { return m.vertexmatrix().count() }
  count(Field f) { return f.__linearize().count() }

  // Obtain energies and constraints from the problem 
  energies() { return self.problem.energies } 
  constraints() { return self.problem.constraints }
  localConstraints() { return self.problem.localconstraints }

  evaluateFunctional(func, method, ...args) { 
    if (func.selection) args.append(func.selection)
    var result = apply(method, args)
    if (isnil(result)) _OptEvlFled.throw("Evaluation of functional ${method} failed.")
    if (func.prefactor) result*=func.prefactor
    return result
  }

  checkFunctionalDependency(func, Field f) {
    var deps 
    if (func.functional.has("field")) deps = func.functional.field 
    if (deps==f) return true 
    if (islist(deps) && (deps.contains(f))) return true 
    return false 
  }

  value() { // Returns the current value of the objective function at the current parameters
    var energy = 0
    for (en in self.energies()) energy+=self.valueForFunctional(en)
    return energy
  }

  valueForFunctional(func) { return self.evaluateFunctional(func, func.functional.total, self.problem.mesh) }

  integrandForFunctional(func) { return self.evaluateFunctional(func, func.functional.integrand, self.problem.mesh) }

  gradient() { // Returns the gradient of the objective function at the current parameters as a column vector
    var l = []
    for (t in self.target) {
      var grad=0
      for (en in self.energies()) {
        var g = self.gradientForFunctional(en, t)
        if (g) grad+=g
      }
      l.append([self.columnVector(grad)])
    }
    if (l.count()==1) return l[0][0]
    return Matrix(l)
  }

  gradientForFunctional(func, Mesh m) {
    return self.evaluateFunctional(func, func.functional.gradient, m)
  }

  gradientForFunctional(func, Field f) {
    if (!func.functional.respondsto("fieldgradient")) return nil
    if (!self.checkFunctionalDependency(func, f)) return nil 
    return self.evaluateFunctional(func, func.functional.fieldgradient, f, self.problem.mesh)
  }

  hessian() { return nil }            // (Optional) Returns the hessian of the objective function at the current parameters as a matrix

  countConstraints() { // Returns number of constraints present
    return self.constraints().count() + self.localConstraints().count() 
  } 

  countEqualityConstraints() { // Returns number of equality constraints present
    return self.countConstraints() - self.countInequalityConstraints() 
  } 
  
  countInequalityConstraints() { // Returns number of inequality constraints present
    var n=0
    for (c in self.constraints()) if (c.has("onesided") && c.onesided) n+=1
    for (c in self.localConstraints()) if (c.has("onesided") && c.onesided) n+=1
    return n 
  } 

  constraintValue() {  // Returns the value(s) of any constraints 
    var a = self.globalConstraintValue()
    var b = self.localConstraintValue()
    var c = self.globalConstraintValue(onesided=true)
    var d = self.localConstraintValue(onesided=true)
    return a.join(b).join(c).join(d)
  }

  _checkOneSided(cons, onesided) {
    return ((onesided && cons.onesided) || (!onesided && !cons.onesided)) 
  }

  globalConstraintValue(onesided=false) {
    var cval=[]
    for (cons in self.constraints()) {
      if (!self._checkOneSided(cons, onesided)) continue

      var val = self.valueForFunctional(cons)
      if (cons.has("target")) val-=cons.target

      cval.append(val)
    }
    return cval
  }

  _reduceIntegrandWithSelection(val, sel) { // Reduces an integrand using a selection
    var l = []
    for (id in sel.idlistforgrade(0)) { // TODO: Handle non-point constraints
      l.append(val[id])
    }  
    return Matrix(l) 
  } 

  _zeroInactiveVal(val) { // Zero out any constraints that are in the feasible region
    for (v, k in val) {
      if (v>0) val[k]=0
    }
  }

  localConstraintValue(onesided=false) {
    var cval=[]
    for (cons in self.localConstraints()) { 
      if (!self._checkOneSided(cons, onesided)) continue

      var val = self.integrandForFunctional(cons).transpose() 
      if (cons.has("target")) val-=cons.target
      if (cons.onesided) self._zeroInactiveVal(val)
      if (cons.selection) val = self._reduceIntegrandWithSelection(val, cons.selection)

      cval.append(val)
    }
    return cval
  }

  _zeroGradient(x) {
    return Matrix(self.count(x)) 
  }

  globalConstraintGradient(onesided=false) {
    var cval=[]
    for (cons in self.constraints()) {
      if (!self._checkOneSided(cons, onesided)) continue

      var l = []
      for (t in self.target) {
        var grad = self.gradientForFunctional(cons, t)

        if (grad) l.append([self.columnVector(grad)])
        else l.append([self._zeroGradient(t)])
      }
      if (l.count()==1) cval.append(l[0][0])
      else cval.append(Matrix(l))
    }
    return cval
  }

  _dofPerEntry(Mesh m) {
    return m.vertexmatrix().dimensions()[0] 
  }

  _dofPerEntry(Field f) {
    var prototype = f[0]
    if (isobject(prototype)) return prototype.count() 
    return 1
  } 

  _element(Matrix m, id) {
    return m.column(id)
  }

  _element(Field f, id) {
    var v = f[0,id]
    if (ismatrix(v)) return v 
    return [v]
  }

  _assembleLocalConstraintGradient(cons, target) {
    var grad = self.gradientForFunctional(cons, target)
    var ndof = self._dofPerEntry(target), ncol = Int(self.count(target)/ndof)

    var val
    if (cons.onesided) val = self.integrandForFunctional(cons) // Only needed if onesided

    var ids 
    if (cons.selection) { // TODO: Handle other than point constraints
      ids = cons.selection.idlistforgrade(0) 
    } else ids = 0...ncol

    var s = Sparse(ndof*ncol, ids.count())

    if (grad) for (id, k in ids) {
      var scale=1
      if (cons.onesided && val[0, id]>0) scale=0

      for (x, j in self._element(grad, id)) s[id*ndof+j,k]=scale*x
    }

    return s
  }

  localConstraintGradient(onesided=false) {
    var cval=[]
    for (cons in self.localConstraints()) {
      if (!self._checkOneSided(cons, onesided)) continue
      
      var l = []
      for (t in self.target) {
        l.append([self._assembleLocalConstraintGradient(cons, t)])
      }

      if (l.count()==1) cval.append(l[0][0])
      else cval.append(Matrix(l))
    }

    return cval
  }

  constraintGradient() { // Returns the gradient(s) of any constraints
    var a = self.globalConstraintGradient()
    var b = self.localConstraintGradient()
    var c = self.globalConstraintGradient(onesided=true)
    var d = self.localConstraintGradient(onesided=true)
    return a.join(b).join(c).join(d)
  }
  
  constraintHessian() { return nil }  // Returns the hessian(s) of any constraints as a list of matrices, or sublists
}
