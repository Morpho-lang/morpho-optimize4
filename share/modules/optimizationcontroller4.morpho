/* ************************************************************
 * OptimizationController
 * ************************************************************ */ 

/* --------------------------
 * Errors
 * -------------------------- */ 

var _OptUnknwnVrbsty = Error("OptUnknwnVrbsty", "Unknown verbosity setting: use 'silent', 'quiet', 'normal', or 'verbose'.")
var _OptMaxErr = Error("OptMaxIter", "Maximum iterations exceeded: present solution does not meet convergence criteria.")
var _OptInf = Error("OptInf", "Optimizer encountered an infinite or non-numerical value.")

var _OptLnSrchDrn = Error("OptLnSrchDrn", "Linesearch encountered an upward direction.")
var _OptLnSrchStpsz = Error("OptLnSrchStpsz", "Linesearch stepsize has tended to zero.")
var _OptLnSrchZm = Error("OptLnSrchZm", "Maximum iterations exceeded in linesearch interval identification.")

var _OptCons = Error("OptCons", "Method is not appropriate for constrained problems: convert to unconstrained problem or use a different controller.")
var _OptUnCons = Error("OptUnCons", "Method not appropriate for unconstrained problems: Use a different Controller.")
var _OptNoGrad = Error("OptNoGrad", "Method requires a gradient.")
var _OptNoHess = Error("OptNoHess", "Method requires a hessian.")
var _OptHessSng = Error("OptHessSng", "Hessian is singular.")

/* --------------------------
 * Reporting levels 
 * -------------------------- */ 

var _OptSilent  = 0 // Only errors
var _OptQuiet   = 1 // Warnings and errors only
var _OptNormal  = 2 // Normal output including 
var _OptVerbose = 3 // Additional debugging information

var _OptVerbosity = { "silent" : _OptSilent,
                      "quiet" : _OptQuiet,
                      "normal" : _OptNormal, 
                      "verbose" : _OptVerbose }

/* --------------------------
 * OptimizationController 
 * -------------------------- */ 

/* OptimizationController is a base class that implements an 
   optimization algorithm or a subcomponent that works by invoking
   the interface of an OptimizationAdapter. */ 

class OptimizationController is DelegateAdapter {
  init(adapter, quiet=false, verbosity=nil) { 
    super.init(adapter)
    
    // Fit a ProxyAdapter over the regular adapter if not present to avoid repeated calculations
    if (adapter.clss()!=ProxyAdapter) self.adapter = ProxyAdapter(adapter)

    // Control verbosity 
    self.verbosity = _OptNormal
    if (quiet) self.verbosity = _OptQuiet
    if (verbosity) {
      if (_OptVerbosity.contains(verbosity)) self.verbosity = _OptVerbosity[verbosity]
      else if (isint(verbosity)) self.verbosity = verbosity
      else _OptUnknwnVrbsty.throw() 
    }

    // Tolerances
    self.gradtol = 1e-6 // Convergence criterion based on norm of the gradient
    self.etol = 1e-8 // Convergence criterion based on change in value of objective function
    self.ctol = 1e-10 // Constraint tolerance

    self._valuehistory=[] // History of objective function values 
  } 

  /* Convenience methods to calculate quantities */

  value() { // Value of the objective function
    var v = self.adapter.value()
    if (isinf(v) || isnan(v)) _OptInf.throw() 
    return v
  }

  countDOF() { // Count degrees of freedom
    return self.adapter.get().count()
  }

  /* Verbosity */

  checkVerbosity(level) { // Checks if output should be printed at a given level
    return (self.verbosity>=level)
  }

  warning(error) { // Generate a warning 
    if (self.checkVerbosity(_OptQuiet)) error.warning() 
  }

  /* Convergence check */

  hasConverged() {
    // Convergence in value  |delta e|/|e| < etol or |delta e| < etol if e is < etol
    if (self._valuehistory && self._valuehistory.count()>1) {
      var f1=self._valuehistory[-1], f2=self._valuehistory[-2]

      // Compute relative change in value, or use absolute value if close to zero
      var de = abs(f1-f2)
      if (abs(f1)>self.etol) de/=(abs(f1))
      if (de < self.etol) return true 
    } 

    var g = self.gradient()
    if (g && g.norm()<self.gradtol) return true 

    return false 
  }

  /* Reporting */

  reportOptimality() { // Report value of optimality condition
    var g = self.gradient()
    if (!g) return ""
    return "|grad|=${g.norm()}"
  }

  reportStepsize() { return "" } // Report stepsize taken

  report(iter) { // Reporting
    if (!self.checkVerbosity(_OptNormal)) return 

    var f = self.value() 

    print "Iteration ${iter.format("%2i")}: ${f} ${self.reportOptimality()} ${self.reportStepsize()}" 
  }

  /* Optimization sequence */

  start() { }           // Initialization at the beginning of an optimization
  begin() { }           // Calculate initial information for the iteration
  step() { }            // Perform optimization step 
  next() { }            // Calculate updated information

  record() {            // Record information about the iteration
    self._valuehistory.append(self.value())
  }

  iterate() {  // Perform one optimization iteration
    self.begin()  
    self.step()
    self.next()
  }

  optimize(nsteps) { // Optimization loop
    self.start()
    self.record() 
    for (i in 1..nsteps) {
      if (self.hasConverged()) return 
      self.iterate() 
      self.report(i)
      self.record()
    }

    self.warning(_OptMaxErr)
  }

  isConstrained() { // Checks if a problem is constrained
    return self.adapter.countConstraints() > 0
  }
}

/* ************************************************************
 * Line search methods
 * ************************************************************ */ 

/* -----------------------------------------
 * GradientDescentController
 * 
 * Simple gradient descent at fixed stepsize 
 * ----------------------------------------- */ 

class GradientDescentController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1) { 
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    if (self.adapter.countConstraints()>0) _OptCons.throw()
    self.stepsize=stepsize
    self._direction=nil 
  }

  // Define a descent direction
  setDirection(d) { 
    self._direction = d     
    var ndof = self.countDOF()
    if (d.count()>ndof) self._direction = d[0...ndof]
  }

  direction() { return self._direction}

  searchDirection() { // Determine the search direction
    self.setDirection(-self.gradient())
  }

  begin() { self.searchDirection() }

  step() { // Take a step in the current search direction
    var x = self.get() 
    x+=self.stepsize*self.direction()
    self.set(x)
  }

  reportStepsize() { return "stepsize=${self.stepsize}" }
}

/* -----------------------------------------
 * LineSearchController
 * 
 * Armijo linesearches
 * ----------------------------------------- */ 

class LineSearchController is GradientDescentController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1, alpha=0.2, beta=0.5) {
    super.init(adapter, quiet=quiet, verbosity=verbosity, stepsize=stepsize)
    self.maxsteps = 50 // Maximum steps for reducing stepsize
    self.alpha = alpha // } Coefficients 
    self.beta = beta   // }
  }

  expectedDescent() { // Predict expected descent
    var d = self.direction()
    var g = self.gradient() 

    self.df = g.inner(d)

    if (self.df>0) self.warning(_OptLnSrchDrn)
  }

  accept(t) { // Have we descended sufficiently?
    return self.value() < self._ovalue + self.alpha*t*self.df
  }

  stepwith(x0, t) { // Take a step of size t from x0 in the descent direction
    self.set(x0 + t*self.direction())
    return self.value()
  }

  step() { // Perform a single linesearch
    self.expectedDescent()
    var t=1
    var success=false
    self._ovalue = self.value() // Save value for comparison

    var x0 = self.get() 
    for (nsteps in 1..self.maxsteps) {
        self.stepwith(x0, t)
        success=self.accept(t)
        if (success) break
        t*=self.beta
    }
    if (!success) self.warning(_OptLnSrchStpsz)

    self.stepsize=t
  }
}

/* -----------------------------------------
 * DirectedLineSearchController
 * 
 * Armijo linesearches in a specified direction
 * ----------------------------------------- */ 

class DirectedLineSearchController is LineSearchController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1, alpha=0.2, beta=0.5, direction=nil) {
    super.init(adapter, quiet=quiet, verbosity=verbosity, stepsize=stepsize, alpha=alpha, beta=beta)
    if (direction) self.setDirection(direction)  
  }
  
  expectedDescent() { // Predict expected descent
    var d = self.direction()
    self.df = self.adapter.directionalDerivative(d) 
    if (self.df>0) self.warning(_OptLnSrchDrn)
    return self.df
  }

  hasConverged() { return false } // Prevent evaluation of gradient

  optimize(n) { // Should only ever perform one step
    self.step()
    self.record()
  }
}

/* -----------------------------------------
 * WolfeLineSearchController
 * 
 * Linesearches that satisfy strong Wolfe 
 * conditions [Nocedal&Wright Chapter 3, p60]
 * ----------------------------------------- */ 

// See also SciPy implementation https://indrag49.github.io/Numerical-Optimization/line-search-descent-methods.html#selection-of-step-length
// See also an implementation at https://github.com/gjkennedy/ae6310/blob/master/Line%20Search%20Algorithms.ipynb

class WolfeLineSearchController is LineSearchController {
  init(adapter, quiet=false, verbosity=nil, stepsize=1, steplimit=2, c1=1e-3, c2=0.9) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    self.maxsteps = 50 // Maximum steps for reducing stepsize
    self.stepsize = stepsize   // Initial stepsize
    self.steplimit = steplimit // Stepsize limit
    self.c1 = c1 // } Coefficients 
    self.c2 = c2 // }
    self.verbose = self.checkVerbosity(_OptVerbose)
  }

  _interpolate(f0, df, alpha, fa) {
    return - df*alpha^2/(2*(fa-f0-df*alpha))
  }

  _zoom(alphalo0, alphahi0, x0, f0, df) {
    var alphalo = alphalo0, alphahi = alphahi0
    var flo = self.stepwith(x0, alphalo)

    for (_ in 1..self.maxsteps) {
      if (self.verbose) print "  Zoom iteration ${_}: (${alphalo},${alphahi}) [${flo}, ${self.stepwith(x0, alphahi)}]"
      var alpha = (alphalo+alphahi)/2 // Bisection

      var f = self.stepwith(x0, alpha)

      if (f>f0+self.c1*alpha*df || // Armijo condition
          f >= flo) {
            if (self.verbose) print "    Armijo test failed; reduce stepsize"
            alphahi = alpha 
      } else {
        self.gradient() 
        var dfalpha = self.gradient().inner(self.direction())

        if (self.verbose) print "    Testing curvature condition: ${abs(dfalpha)} <= ${self.c2*abs(df)}"
        if (abs(dfalpha) <= self.c2*abs(df)) { // Curvature condition
          if (self.verbose) print "    Curvature condition succeeded alpha=${alpha.format("%.16g")}"
          self.stepsize = alpha 
          return 
        } else if (dfalpha*(alphahi-alphalo)>=0) {
          if (self.verbose) print "    Swap intervals"
          alphahi = alphalo 
        }

        if (self.verbose) print "    Increase stepsize "
        alphalo = alpha  
        flo = f 
      }
    }
 
    self.warning(_OptLnSrchZm)
  }

  step() { // Perform a single linesearch; Alg. 3.5 of N&D
    self.expectedDescent()

    var f0 = self.value()
    var df = self.df 
    var x0 = self.get() 

    var alpha=self.stepsize
    var oalpha=0
    var of = f0 

    for (_ in 1..self.maxsteps) {
      var f = self.stepwith(x0, alpha)
      if (self.verbose) print "  ls iteration ${_}: alpha=${alpha} f=${f}"

      if (f>f0+self.c1*alpha*df ||
          f>=of) { // Step is too long if Armijo test fails or the function simply increased
        if (self.verbose) print "  Armijo test failed; zooming on interval (${oalpha},${alpha})"
        return self._zoom(oalpha, alpha, x0, f0, df) 
      }
       
      var dfalpha = self.gradient().inner(self.direction())

      if (abs(dfalpha) <= self.c2*abs(df)) { // Curvature test
        if (self.verbose) print "  Curvature test succeeded alpha=${alpha.format("%.16g")}"
        self.stepsize = alpha // Success
        if (self.verbose) print self.adapter.get()
        return 
      }

      if (dfalpha>=0) { // Gradient is upward so reduce stepsize
        if (self.verbose) print "  Upward gradient detected; zooming on interval (${alpha},${oalpha})"
        return self._zoom(alpha, oalpha, x0, f0, df)
      }
      
      of = f 
      oalpha = alpha 

      //alpha = self._interpolate(f0, df, alpha, f)
      alpha = min(2*alpha, self.steplimit)
    }

    if (self.verbose) print "Too many iterations in step."
  }

  _interpolate(x0,x1,f0,f1,df0,df1) { // Cubic interpolation Nocedal p59
    var d1 = df0 + df1 - 3*(f1-f0)/(x1-x0)
    var s = d1^2-df0*df1
    if (s<0) return (x0+x1)/2
    var d2 = sign(x1-x0)*sqrt(s)

    return x1-(x1-x0)*(df1+d2-d1)/(df1-df0-2*d2) // Should check if outside interval
  }
}

/* ************************************************************
 * Newton and quasi-Newton methods
 * ************************************************************ */ 

/* -----------------------------------------
 * NewtonController
 * 
 * Newton's method with linesearches
 * ----------------------------------------- */ 

class NewtonController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, linesearch=nil) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    if (self.adapter.countConstraints()>0) _OptCons.throw()
    if (isclass(linesearch)) self.lscontroller = linesearch(self.adapter, quiet=quiet, verbosity=self.verbosity)
    else if (isobject(linesearch)) self.lscontroller = linesearch
    else self.lscontroller = LineSearchController(self.adapter, verbosity=self.verbosity)
  }

  linesearch(dirn) {
    var x0 = self.adapter.get() /// <- If our space and linesearch space don't coincide
    self.lscontroller.setDirection(dirn)
    self.lscontroller.step() 
    self.adapter.set(x0+self.lscontroller.stepsize*dirn) /// <- If our space and linesearch space don't coincide
  }

  direction() {
    var g = self.gradient() 
    var H = self.hessian()
    if (!H) _OptNoHess.throw("NewtonController can't use ${self.adapter.clss()} because it doesn't provide a hessian.") 
    return -g/H
  }

  step() { 
    var d = self.direction()
    self.linesearch(d)
  }

  reportstepsize() { return "stepsize=${self.lscontroller.stepsize}" }
}

/* -----------------------------------------
 * ConjugateGradientController
 * 
 * Conjugate gradient with linesearch
 * ----------------------------------------- */ 

class ConjugateGradientController is NewtonController {
  start() {
    self._odirection = nil 
    self._ogradient = nil
  }

  direction() {
    var g = self.gradient() 
    self._gradient = g 
    self._direction = -g

    if (self._ogradient) {
      var beta = g.inner(g) / self._ogradient.inner(self._ogradient) 
      if (beta<0) return self._direction
      self._direction += beta*self._odirection
    }
     
    return self._direction
  }

  next() {
    self._ogradient = self._gradient 
    self._odirection = self._direction 
  }
}

/* -----------------------------------------
 * BFGSController
 * 
 * BFGS with dense matrices
 * ----------------------------------------- */ 

class BFGSController is NewtonController {
  hessian() { // Return our estimate of the hessian
    return self.B
  }

  start() {
    self.B = IdentityMatrix(self.get().count())
  }

  begin() {
    super.begin() 
    self._ox = self.get() 
    self._ogradient = self.gradient() 
  }

  update(sk, yk) {
    var u = self.B*sk 
    var yksk = yk.inner(sk)
    var sku = sk.inner(u)
    if (yksk==0 || sku==0) { self.start(); return }

    self.B += yk*yk.transpose()/yksk - u*u.transpose()/sku
  }

  next() {
    var sk = self.get() - self._ox
    var yk = self.gradient() - self._ogradient

    self.update(sk,yk)
  }
}

/* -----------------------------------------
 * InvBFGSController
 * 
 * BFGS but estimating the inverse hessian
 * ----------------------------------------- */ 

class InvBFGSController is BFGSController {
  start() {
    super.start() 
    self.I = IdentityMatrix(self.get().count())
  }

  direction() {
    var g = self.gradient()
    return -self.B*g 
  }

  _hmul(p) {
    return self.B*p
  }

  update(sk, yk) {
    var v = yk.inner(sk)
    if (v==0) { self.B=self.I; return }
    var U = self.I - yk*sk.transpose()/v

    self.B = U.transpose()*self.B*U + sk*sk.transpose()/v
  }

  next() {
    var sk = self.get() - self._ox
    var yk = self.gradient() - self._ogradient

    self.update(sk, yk)
  }
}

/* -----------------------------------------
 * LBFGSController
 * 
 * Limited memory variant of BFGS
 * ----------------------------------------- */ 

class LBFGSController is InvBFGSController {
  init(adapter, quiet=false, verbosity=nil, maxhistorylength=10) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    self.maxhistorylength = maxhistorylength
  }

  _popfirst(lst) { // Pops the first element of a list
    var a = lst.roll(-1)
    a.pop() 
    return a 
  }

  _hmul(p) { // Compute the action of our approximation to the inv H on a vector or matrix 
    var q = p

    var n = self._storage.count() 
    if (n==0) return q
    
    var alpha[n]

    // Project off all components of p that lie in the history directions
    for (k in n-1..0:-1) { // Start with most recent
      var z = self._storage[k]
      var sk = z[0], yk = z[1], rho = 1/z[2]
      if (isinf(rho)) _OptHessSng.throw() 

      alpha[k] = rho * sk.transpose()*q //sk.inner(q)
      q-=yk*alpha[k]
    } 

    var zl = self._storage[-1] // Last 
    var r = (zl[2]/zl[3])*q // (sk.yk)/(yk.yk)
    if (zl[3]==0) _OptHessSng.throw() 

    for (z, k in self._storage) { // Work forwards
      var sk = z[0], yk = z[1], rho = 1/z[2]
      if (isinf(rho)) _OptHessSng.throw() 

      var beta = rho * yk.transpose()*r// rho*yk.inner(r)
      r+=sk*(alpha[k]-beta)
    }

    return r 
  }

  start() {
    self._storage = []
  }

  direction() {
    return -self._hmul(self.gradient())
  }

  update(sk, yk) {
    self._storage.append((sk, yk, sk.inner(yk), yk.inner(yk)))

    if (self._storage.count()>self.maxhistorylength) self._storage = self._popfirst(self._storage)
  }

  next() { // Update our estimate of the inverse Hessian
    var sk = self.get() - self._ox
    var yk = self.gradient() - self._ogradient

    self.update(sk, yk) 
  }
}

/* ************************************************************
 * Controllers for constrained optimization
 * ************************************************************ */ 

/* -----------------------------------------
 * PenaltyController
 * ----------------------------------------- */ 

/** Implements a penalty method. Converts a constrained problem into an unconstrained problem 
    using a PenaltyAdapter. Solves a sequence of subproblems with increasing penalty parameter
    until convergence criteria and constraint goal has been achieved. */

class PenaltyController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, controller=nil, mu0=1, mumul=10.0) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)

    if (!self.isConstrained()) _OptUnCons.throw()

    self.mumul = mumul // Factor to multiply penalty by at each iteration

    self.adapter = PenaltyAdapter(self.adapter)
    self.adapter.setpenalty(mu0)

    self.controller = controller 
    if (!controller) self.controller = LBFGSController
  }

  begin() {
    self.mu=self.adapter.penalty()
    self.control = self.controller(self.adapter, verbosity=self.verbosity)
    self.control.etol = self.etol 
    self.control.gradtol = self.gradtol 
  }

  start() {
    self.control = nil 
  }

  step() {
    self.control.optimize(100) 
  }

  next() {
    self.adapter.setpenalty(self.mumul*self.adapter.penalty())
  }

  constraintNorm() {
    return self.adapter.constraintVector().norm(1)
  }

  hasConverged() {
    if (!self.control) return false 
    return self.control.hasConverged() && (self.constraintNorm()<self.ctol)
  }

  report(iter) {
    if (!self.checkVerbosity(_OptNormal)) return 
    
    print "==Penalty iteration ${iter}: mu=${self.mu} |constraints|=${self.constraintNorm()}"
  }
}

/* -----------------------------------------
 * ProjectedGradientDescentController
 * ----------------------------------------- */ 

/** Implements projected gradient descent. Performs gradient descent steps with linesearches: 
    * Subtracts off any component of the gradient in the constrained direction to help maintain feasibility
    * Line search uses an L1PenaltyAdapter as a merit function
    * Performs a reprojection step after each iteration */

class ProjectedGradientDescentController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, maxconstraintsteps=100, mu=2, steplimit=nil) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    self.maxconstraintsteps=maxconstraintsteps
    if (!self.isConstrained()) _OptUnCons.throw()
    self.mu = mu
    self.steplimit = steplimit
    self._gradient = nil 
  }

  reproject() { // Perform reprojection
    var opt = LBFGSController(self.rpadapter, quiet=true)
    opt.optimize(self.maxconstraintsteps)
  }

  start() {
    self.rpadapter = ReprojectionAdapter(self.adapter)
    reproject()
  }

  _eliminate(A, ix) { // Eliminate variable ix by zeroing out the associated row and column, replace with 1 on diagonal
    var dim = A.dimensions()[0]
    for (i in 0...dim) {
      A[i,ix]=0
      A[ix,i]=0
    }
    A[ix,ix]=1
  }

  _checkInequalityConstraints(A, b) {
    var cv = self.adapter.constraintValue()
    var cvec = self.rpadapter._flatten(cv)
    var neq = self.rpadapter._counteq(cv)

    for (i in neq...cvec.count()) {
      if (cvec[i]>=0) { 
        b[i]=0
        _eliminate(A, i)
      }
    }
  }

  direction() {
    var g = self.adapter.gradient()
    var dg = self.adapter.constraintGradient()   
    var C = Matrix([dg])
    var Ct = C.transpose()

    var Ctg = Ct*g
    var CtC = Ct*C

    // Check for inactive constraints
    if (self.adapter.countInequalityConstraints()>0) _checkInequalityConstraints(CtC, Ctg)

    // Project off the constraint directions
    var lambda=Ctg/CtC
    g -= C*lambda
    self._gradient = g    

    return -g // Return the downward direction
  }

  linesearch(dirn) {
    var x0 = self.adapter.get() 
    var mu = self.mu 
    var padapt = L1PenaltyAdapter(self.adapter, penalty=mu)
    var ls = DirectedLineSearchController(padapt, direction=dirn)
    ls.step()
    if (self.steplimit && ls.stepsize>self.steplimit) {
      self.adapter.set(x0 + self.steplimit*ls.direction())
    }
  }

  step() {
    var d = direction() // Search direction 
    linesearch(d)
    reproject() 
  }

  gradient() { // Ensure we report most recent value of the projected gradient
    if (isnil(self._gradient)) self.direction() 
    return self._gradient
  }

  reportStepsize() { 
    return "|constraints|=${self.rpadapter.constraintVector().norm(1)}" 
  }
}

/* -----------------------------------------
 * SQPController
 * ----------------------------------------- */ 

/** Implements Sequential Quadratic Programming. 
    Converts the problem to an unconstrained problem using a LagrangeMultiplierAdapter
    Uses LBFGS to maintain an estimate of the hessian
    Search direction is chosen from LBFGS estimate of the kkt matrix
    Line search uses an L1PenaltyAdapter as a merit function
     */

class SQPController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, penalty=1) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)

    if (!self.isConstrained()) _OptUnCons.throw()

    self.penalty = penalty // Initial penalty for linesearch
  }

  gradient() { // Ensure the gradient of the Lagrangian is reported
    return self.ladapter.gradient()
  }

  start() {
    self.ladapter = LagrangeMultiplierAdapter(self.adapter)
    self.lbfgs = LBFGSController(DeconstrainAdapter(self.adapter))
    self.lbfgs.start() 
  }

  begin() {
    self._ox = self.get() 
    self._ogradient = self.ladapter.varGradient() 
  }

  invHessian() {
    var x = self.adapter.get() 
    var n = x.count() 
    var h = []
    for (i in 0...n) {
      var xi = Matrix(n); xi[i]=1
      h.append(self.lbfgs._hmul(xi))
    }

    return Matrix([h]) 
  }

  kkt() { // Estimate kkt matrix explicitly
    var h = self.invHessian().inverse() // We have to reconstruct the inv. hessian and invert it

    var cg = self.adapter.constraintGradient()
    var C = Matrix([cg])
    var Ct = C.transpose()

    return Matrix([[h,-C],[-Ct,0]]) 
  }

  solve() {
    // Evaluate all elements of the KKT matrix
    var g = self.ladapter.varGradient() 

    var Hg = self.lbfgs._hmul(g) 

    var c = self.ladapter.activeConstraintValue()

    if (c.count()==0) { // Special case if the active set is empty
      return -Matrix([ [ Hg ],
                       [ 0*self.ladapter.lagrangeMultipliers() ] ])
    }

    var cg = self.ladapter.activeConstraintGradient()

    var C = Matrix([cg])
    var Ct = C.transpose()

    // Form the Schur complement
    var HC = self.lbfgs._hmul(C)
    var S = Ct*HC 

    var Sinvc = c/S
    var SinvCt = (Ct*Hg)/S

    var HCSinvCt = self.lbfgs._hmul(C*SinvCt)
    var HCCSc = self.lbfgs._hmul(C*Sinvc)

    var dl = self.ladapter.activeUpdate(- SinvCt + Sinvc)

    return -Matrix([ [Hg - HCSinvCt + HCCSc],
                     [dl] ])
  }

  _choosePenalty(dirn) {
    var maxpenalty=max(self.ladapter.lagrangeMultipliers())
    if (maxpenalty>self.penalty) self.penalty = 2*maxpenalty

    return self.penalty
  }

  linesearch(dirn) { // Perform a linesearch on the original constrained problem, returning the stepsize
    var x0 = self.adapter.get()
    var ndof = x0.count() 
    var xdirn = dirn[0...ndof]

    var mu = self._choosePenalty(xdirn)

    var padapt = L1PenaltyAdapter(self.adapter, penalty=mu)

    var ls = DirectedLineSearchController(padapt, direction=xdirn)
    ls.step()

    self.adapter.set(x0)

    return ls.stepsize
  }

  step() {
    var x0 = self.ladapter.get()
    var ndof = self.adapter.get().count()

    var d = self.solve() // Find search direction
    var alpha = self.linesearch(d) // Find stepsize

    var dl = d.clone() // Evaluate gradient at new lagrange multipliers
    for (i in 0...ndof) dl[i]=0
    self.ladapter.set(x0 + alpha*dl)
    self._ogradient = self.ladapter.varGradient() 

    self.ladapter.set(x0 + alpha*d) // New set new value

    self.stepsize = alpha 
  }

  constraintNorm() {
    return PenaltyAdapter(self.adapter).constraintVector().norm(1)
  }

  reportStepsize() { 
    return "|constraints|=${self.constraintNorm()}" 
  }

  next() {
    // Update LBFGS estimate of inverse hessian
    // Note that we choose yk = grad L(x+alpha d, lambda k+1)-grad L(x, lambda k+1)
    // i.e. holding lambda constant
    var sk = self.get() - self._ox
    var yk = self.ladapter.varGradient() - self._ogradient

    self.lbfgs.update(sk, yk)
  }
}
