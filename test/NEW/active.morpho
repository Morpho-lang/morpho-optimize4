// Active set method

import optimize4 

var _OptConsCountErr = Error("OptConsCount", "More equality constraints specified than provided.")
var _OptUnConsErr = Error("OpUncons", "Problem is unconstrained; this adapter is intended for constrained problems.")
var _OptNoHessErr = Error("OptNoHess", "Method requires a hessian.")

var _OptNoActiveFuncErr = Error("OptNoActiveFunc", "Problem has no active functionals.")
var _OptTrgtLstEmptyErr = Error("OptTrgtLstEmpty", "List of optimization targets is empty.")

var _OptNoPrblmErr = Error("OptNoPrblm", "ProblemAdapter must be initialized with an OptimizationProblem.")
var _OptPrblmTrgtErr = Error("OptPrblmTrgt", "ProblemAdapter must be initialized with Meshes or Fields as targets.")


/* Adapts a provided function (and an optional gradient and hessian) 
   to the OptimizationAdapter interface */

var _epsfd=(1e-16)^(1/3) // Optimal relative stepsize for cell centered gradient
var _epshess=(1e-16)^(1/4) // Optimal relative stepsize for cell centered hessian

class XFunctionAdapter is OptimizationAdapter {
  init(target, gradient=nil, hessian=nil, start=nil, 
               constraints=nil, constraintgradients=nil, constrainthessians=nil,
               equalitycount=nil) {
    super.init(target)
    self.x = start
    
    self.gradfn = gradient 
    if (!gradient) self.gradfn = self._numericalgrad(self.target) // Use FD if no grad provided

    self.hessianfn = hessian
    if (!hessian) self.hessianfn = self._numericalhessian(self.target) // Use FD if no hessian provided

    self.constraints = constraints

    self.constraintcount = 0 
    if (constraints) self.constraintcount = self.constraints.count()
    self.equalitycount = self.constraintcount // By default, all constraints are equality constraints
    if (equalitycount) self.equalitycount = equalitycount

    if (self.equalitycount>self.constraintcount) _OptConsCountErr.throw()

    self.constraintgradfn = constraintgradients
    if (self.constraints && !self.constraintgradfn) {
      var lst = []
      for (c in self.constraints) lst.append(self._numericalgrad(c))
      self.constraintgradfn = lst 
    }

    self.constrainthessianfn = constrainthessians
    if (self.constraints && !self.constrainthessianfn) {
      var lst = []
      for (c in self.constraints) lst.append(self._numericalhessian(c))
      self.constrainthessianfn = lst 
    }
  }

  _listify(x) { // Converts an enumerable to a list
    var l = []
    for (e in x) l.append(e)
    return l 
  }

  _stepsize(eps, x) { // Select stepsize for finite differences
    var h=eps
    var xx=abs(x)
    if (xx>1) h*=xx // Scale

    var temp = x+h  // Ensure we obtain an FP representable number
    return temp - x 
  }

  _numericalgrad(f) { // Construct a numerical gradient as a closure 
    fn numericalgradfn(...x) {
      var n = x.count()
      var grad = Matrix(n)

      for (var i=0; i<n; i+=1) {
        var x0 = x[i]
        var eps = self._stepsize(_epsfd, x0)
        x[i]=x0+eps
        var fr = apply(f, x)
        x[i]=x0-eps
        var fl = apply(f, x)
        grad[i]=(fr-fl)/(2*eps)
      }

      return grad 
    }
    return numericalgradfn
  }

  _numericalhessian(f) {
    fn numericalhessianfn(...x) {
      var n = x.count()
      var hess = Matrix(n,n)
      var f0 = apply(f, x)

      for (var i=0; i<n; i+=1) {
        var x0 = x[i]
        var epsx = self._stepsize(_epsfd, x0)

        // Diagonal entries see Abramowitz and Stegun 1972, p. 884, 25.3.23, O(h^2)
        x[i]=x0+epsx
        var fr = apply(f, x)
        x[i]=x0-epsx
        var fl = apply(f, x)
        x[i]=x0

        hess[i,i]=(fr+fl-2*f0)/(epsx*epsx)

        // Off diagonal entries Abramowitz and Stegun 1972, p. 884, 25.3.26 O(h^2)
        for (var j=0; j<i; j+=1) {
          var y0 = x[j]            
          var epsy = self._stepsize(_epsfd, y0)

          x[i]=x0+epsx; x[j]=y0+epsy
          var frr = apply(f, x)
          x[i]=x0-epsx; x[j]=y0-epsy
          var fll = apply(f, x)
          x[i]=x0-epsx; x[j]=y0+epsy
          var flr = apply(f, x)
          x[i]=x0+epsx; x[j]=y0-epsy
          var frl = apply(f, x)
          x[i]=x0; x[j]=y0

          hess[i,j]=hess[j,i]=(frr+fll-flr-frl)/(epsx*epsy)/4
        }
      }

      return hess  
    } 
    return numericalhessianfn 
  }

  set(x) { self.x = x }
  get() { return self.x }

  value() { return apply(self.target, self._listify(self.x)) }
  gradient() { return apply(self.gradfn, self._listify(self.x)) }
  hessian() { return apply(self.hessianfn, self._listify(self.x)) }

  countConstraints() { return self.constraintcount }
  countEqualityConstraints() { return self.equalitycount }

  constraintMap(list) {
    var val = []
    for (c, k in list) val.append( apply(c, self._listify(self.x)) )
    return val 
  } 

  constraintValue() { 
    if (!self.constraints) return nil 
    return self.constraintMap(self.constraints)
  }   

  constraintGradient() { 
    if (!self.constraints) return nil 
    return self.constraintMap(self.constraintgradfn)
  }

  constraintHessian() { 
    if (!self.constraints) return nil 

    if (!self.constraints) return nil 
    return self.constraintMap(self.constrainthessianfn)
  }
}

fn func(x, y) {
  return x + y
}

fn c(x, y) {
  return 2 - x^2 - y^2 
}

fn d(x, y) {
  return y
}

var adapter = XFunctionAdapter(func, start=Matrix([0, 0]), constraints=[c, d], equalitycount=0)

adapter.set(Matrix([0,-0.4]))
//print adapter.constraintValue()

var _OptNonDiffErr = Error("OptNonDiff", "Objective function is nondifferentiable; use a Controller that avoids differentiation.")

/* Converts a constrained optimization problem into an unconstrained one 
   using an L1 norm: 
      F = f + mu*|c| + mu*|d-| 
   The resulting function is nondifferentiable, but does admit a 
   directional derivative. 
   Commonly used as a merit function. */

class L1PenaltyAdapter is PenaltyAdapter {
  value() { // F = f + mu*|c| + mu*|d-|
    var f = self.adapter.value() 
    var c = self.constraintVector()  

    if (c) f += self.penalty * c.norm(1)

    return f
  }

  gradient() { _OptNonDiffErr.throw() }
  hessian() { _OptNonDiffErr.throw() }

  directionalDerivative(d) {
    var df = self.adapter.gradient() 
    var c = self.constraintVector()

    return df.inner(d) - self.penalty*c.norm(1)
  }
}


var padapt = L1PenaltyAdapter(adapter, penalty=10) 

print padapt.value() 

print padapt.directionalDerivative(Matrix([0,1])) 

var _OptLnSrchDrn = Error("OptLnSrchDrn", "Linesearch encountered an upward direction.")
var _OptLnSrchDrn = Error("OptLnSrchDrn", "Linesearch encountered an upward direction.")

/** Linesearch controller that searches in a given fixed direction */
class DirectedLineSearchController is LineSearchController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1, alpha=0.2, beta=0.5, direction=nil) {
    super.init(adapter, quiet=quiet, verbosity=verbosity, stepsize=stepsize, alpha=alpha, beta=beta)
    if (direction) self.setDirection(direction)  
  }
  
  expectedDescent() { // Predict expected descent
    var d = self.direction()
    self.df = self.adapter.directionalDerivative(d) 
    if (self.df>0) self.warning(_OptLnSrchDrn)
  }

  hasConverged() { return false } // Prevent evaluation of gradient

  optimize(n) { // Should only ever perform one step
    self.step()
    self.record()
  }
}

var control = DirectedLineSearchController(padapt, direction=Matrix([0,1]))

print "--"
print adapter.value() 
print padapt.value() 
print padapt.get() 
control.optimize(1)
print adapter.value() 
print padapt.value() 
print padapt.get() 