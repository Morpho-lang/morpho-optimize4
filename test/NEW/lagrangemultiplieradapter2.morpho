// Lagrange multiplier adapter

import optimize4 

var _OptUnConsErr = Error("OpUncons", "Problem is unconstrained; this adapter is intended for constrained problems.")
var _OptNoHessErr = Error("OptNoHess", "Method requires a hessian.")

class LagrangeMultiplierAdapter is OptimizationAdapter {
  init(adapter) { 
    self.adapter = adapter
    self._initialLagrangeMultipliers() 
  }

  _initialLagrangeMultipliers() { // Initialize lagrange multipliers to 1
    var cv = self.adapter.constraintValue()
    self.lambda = []
    for (c in cv) self.lambda.append(1+0*c)
    self._checkLagrangeMultipliers()
  }

  _checkLagrangeMultipliers() { // Zeros lagrange multipliers for inactive constraints
    var cv = self.adapter.constraintValue() 
    self._checkIneq(cv, self.lambda)
  }

  set(x) { 
    var n = x.count()
    var nvars = self.adapter.get().count()
    self.adapter.set(x[0...nvars,0])
    self.lambda = self._unflatten(x[nvars...n,0], self.lambda) 
    self._checkLagrangeMultipliers()
  }

  get() { // Get parameters, joining real parameters with lagrange multipliers
    var x = self.adapter.get()
    return Matrix([[x], [self._flatten(self.lambda)]]) 
  }

  _unflatten(Matrix v, shape) { // Converts a Vector into a list of scalars and vectors according to a given shape
    var l = []
    var n = 0 
    for (u, k in shape) {
      if (ismatrix(u)) {
        var m = u.count()
        l.append(v[n...n+m,0])
        n+=m
      } else {
        l.append(v[n])
        n+=1
      }
    }

    return l 
  }  

  _flatten(v) { // Flattens a list of scalars and vectors into a Vector
    var l = []
    for (c in v) {
      if (ismatrix(c)) for (x in c) l.append(x)
      else l.append(c)
    }
    return Matrix(l)
  } 

  setLagrangeMultipliers(lambda) {
    self.lambda = self._unflatten(lambda, self.lambda)
  }

  lagrangeMultipliers() {
    return self._flatten(self.lambda)
  }

  _zeroIneq(Matrix m, Matrix out) {
    for (c, k in m) {
      if (c>=0) out[k] = 0
    }
    return out 
  }

  _zeroIneq(u, out) {
    if (u>=0) return 0
    return out
  }

  _checkIneq(c, out) { // Checks whether entries in c are inside the feasible 
                       // region and sets the corresponding entry in out to zero
    var neq = self.adapter.countEqualityConstraints()
    for (k in neq...c.count()) {
      out[k]=self._zeroIneq(c[k], out[k])
    }
  }

  _inner(Matrix u, Matrix v) { return u.inner(v) }
  _inner(u, v) { return u*v }

  inner(c) { // Takes the inner product of the list c with the lagrange multipliers
    var total = 0 
    for (u,k in c) total+=self._inner(self.lambda[k], u)
    return total
  }

  value() { // Lagrangian function = f - lambda_i c_i
    var f = self.adapter.value() 
    var c = self.adapter.constraintValue()
    self._checkIneq(c, c)

    return f - self.inner(c)
  }

  varGradient() { // Gradient of lagrangian wrt original degrees of freedom
    var grad = self.adapter.gradient()
    var cgrad = self.adapter.constraintGradient()

    for (cg, k in cgrad) grad -= cg*self.lambda[k]

    return grad 
  }

  gradient() { // Gradient of Lagrangian is [ df - lambda_i dc_i , -c_i] 
    var c = self.adapter.constraintValue()
    self._checkIneq(c, c)
    var cv = self._flatten(c)

    return Matrix([[self.varGradient()],[-cv]]) 
  } 

  hessian() { // Hessian of Lagrangian is the kkt matrix 
    Error.throw("NOT IMPLEMENTED")
/*    var h = self.adapter.hessian()   
    if (!h) _OptNoHessErr.throw() 

    var chess = self.adapter.constraintHessian() 
    for (ch,k in chess) {
      if (!ch) _OptNoHessErr.throw("Constraint ${ch.clss()} does not provide a hessian.")
      h -= self.lambda[k]*ch
    }

    var cg = self.adapter.constraintGradient()
    var C = Matrix([cg])

    return Matrix([[h, -C],[-C.transpose(), 0]])*/
  }

  _checkConstraintValue(Matrix u, isEq, out) {
    for (c in u) {
      if (isEq || c<0) out.append(c)
    }
  }

  _checkConstraintValue(u, isEq, out) {
    if (isEq || u<0) out.append(u)
  }

  activeConstraintValue() {
    var cv = self.adapter.constraintValue()
    var neq = self.adapter.countEqualityConstraints()
    var l = []
    for (c, k in cv) {
      self._checkConstraintValue(c, k<neq, l)
    }
    return Matrix(l)
  }

  _checkConstraintGradient(Matrix u, grad, isEq, out) {
    if (isEq) {
      out.append(grad)
    } else {
      var l = []
      for (c, k in u) {
        if (isEq || c<0) l.append(grad.column(k))
      }
      out.append(Sparse([l]))
    }
  }

  _checkConstraintGradient(u, grad, isEq, out) {
    if (isEq || u<0) out.append(grad)
  }

  activeConstraintGradient() {
    var cv = self.adapter.constraintValue()
    var cgrad = self.adapter.constraintGradient()
    var neq = self.adapter.countEqualityConstraints()
    var l = []
    for (c, k in cv) {
      self._checkConstraintGradient(c, cgrad[k], k<neq, l)
    }
    return l 
  }

  activeUpdate(dl) { // Promote update onto active set
    var cv = self.adapter.constraintValue()
    var neq = self.adapter.countEqualityConstraints()
    var out = []
    var dli = 0 // Index into dl 
    for (c, k in cv) {
      if (ismatrix(c)) {
        Error.throw("NOT IMPLEMENTED")
      } else {
        if (k<neq || c<0) {
          out.append(dl[dli])
          dli +=1 
        } else out.append(0)
      }
    }
    return Matrix(out)
  }

}
