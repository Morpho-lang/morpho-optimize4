// SQP method

import optimize4 

var _OptUnConsErr = Error("OpUncons", "Problem is unconstrained; this adapter is intended for constrained problems.")
var _OptUnCons = Error("OptUnCons", "Method not appropriate for unconstrained problems: Use a different Controller.")

class SQPController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, penalty=1) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)

    if (!self.isConstrained()) _OptUnCons.throw()

    self.penalty = penalty // Initial penalty for linesearch
  }

  gradient() { // Ensure the gradient of the Lagrangian is reported
    return self.ladapter.gradient()
  }

  start() {
    self.ladapter = LagrangeMultiplierAdapter(self.adapter)
    self.lbfgs = LBFGSController(self.ladapter)
    self.lbfgs.start() 
  }

  begin() {
    self._ox = self.get() 
    self._ogradient = self.ladapter.varGradient() 
  }

  invHessian() {
    var x = self.adapter.get() 
    var n = x.count() 
    var h = []
    for (i in 0...n) {
      var xi = Matrix(n); xi[i]=1
      h.append(self.lbfgs._hmul(xi))
    }

    return Matrix([h]) 
  }

  kkt() { // Estimate kkt matrix explicitly
    var h = self.invHessian().inverse() // We have to reconstruct the inv. hessian and invert it

    var cg = self.adapter.constraintGradient()
    var C = Matrix([cg])
    var Ct = C.transpose()

    return Matrix([[h,-C],[-Ct,0]]) 
  }

  solve() {
    // Evaluate all elements of the KKT matrix
    var g = self.ladapter.varGradient() 
    var c = self.ladapter.activeConstraintValue()
    var cg = self.ladapter.activeConstraintGradient()

    var C = Matrix([cg])
    var Ct = C.transpose()

    // Form the Schur complement
    var HC = self.lbfgs._hmul(C)
    var S = Ct*HC 

    // Apply the formula to invert the KKT system
    var Hg = self.lbfgs._hmul(g)

    var Sinvc = c/S
    var SinvCt = (Ct*Hg)/S

    var HCSinvCt = self.lbfgs._hmul(C*SinvCt)
    var HCCSc = self.lbfgs._hmul(C*Sinvc)

    var dl = self.ladapter.activeUpdate(- SinvCt + Sinvc)

    return -Matrix([ [Hg - HCSinvCt + HCCSc],
                     [dl] ])
  }

  _choosePenalty(dirn) {
    var maxpenalty=max(self.ladapter.lagrangeMultipliers())
    if (maxpenalty>self.penalty) self.penalty = 2*maxpenalty

    return self.penalty
  }

  linesearch(dirn) {
    var x0 = self.adapter.get()
    var ndof = x0.count() 
    var xdirn = dirn[0...ndof]

    var mu = self._choosePenalty(xdirn)

    var padapt = L1PenaltyAdapter(self.adapter, penalty=mu)
    var ls = DirectedLineSearchController(padapt, direction=xdirn)
    ls.step()

    self.adapter.set(x0)

    return ls.stepsize
  }

  step() {
    var x0 = self.ladapter.get()
    var ndof = self.adapter.get().count()

    var d = self.solve() // Find search direction
    var alpha = self.linesearch(d) 

    var dl = d.clone() // Evaluate gradient at new lagrange multipliers
    for (i in 0...ndof) dl[i]=0
    self.ladapter.set(x0 + alpha*dl)
    self._ogradient = self.ladapter.varGradient() 

    self.ladapter.set(x0 + alpha*d) // New set new value

    self.stepsize = alpha 
  }

  constraintNorm() {
    return PenaltyAdapter(self.adapter).constraintVector().norm(1)
  }

  reportStepsize() { 
    return "|constraints|=${self.constraintNorm()}" 
  }

  next() {
    // Note that we choose yk = grad L(x+alpha d, lambda k+1)-grad L(x, lambda k+1)
    // i.e. holding lambda constant
    var sk = self.get() - self._ox
    var yk = self.ladapter.varGradient() - self._ogradient
    self.lbfgs.update(sk, yk)
  }
}