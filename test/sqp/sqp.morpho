// SQP method

import optimize4 

var _OptUnConsErr = Error("OpUncons", "Problem is unconstrained; this adapter is intended for constrained problems.")
var _OptUnCons = Error("OptUnCons", "Method not appropriate for unconstrained problems: Use a different Controller.")

class LagrangeMultiplierAdapter is OptimizationAdapter {
  init(adapter) { 
    self.adapter = adapter
    self._initialLagrangeMultipliers() 
  }

  _initialLagrangeMultipliers() { // Initialize lagrange multipliers to 1
    var cv = self.adapter.constraintValue()
    self.lambda = []
    for (c in cv) self.lambda.append(1+0*c)
    self._checkLagrangeMultipliers()
  }

  _checkLagrangeMultipliers() { // Zeros lagrange multipliers for inactive constraints
    var cv = self.adapter.constraintValue() 
    self._checkIneq(cv, self.lambda)
  }

  set(x) { 
    var n = x.count()
    var nvars = self.adapter.get().count()
    self.adapter.set(x[0...nvars,0])
    self.lambda = self._unflatten(x[nvars...n,0], self.lambda) 
    self._checkLagrangeMultipliers()
  }

  get() { // Get parameters, joining real parameters with lagrange multipliers
    var x = self.adapter.get()
    return Matrix([[x], [self._flatten(self.lambda)]]) 
  }

  _unflatten(Matrix v, shape) { // Converts a Vector into a list of scalars and vectors according to a given shape
    var l = []
    var n = 0 
    for (u, k in shape) {
      if (ismatrix(u)) {
        var m = u.count()
        l.append(v[n...n+m,0])
        n+=m
      } else {
        l.append(v[n])
        n+=1
      }
    }

    return l 
  }  

  _flatten(v) { // Flattens a list of scalars and vectors into a Vector
    var l = []
    for (c in v) {
      if (ismatrix(c)) for (x in c) l.append(x)
      else l.append(c)
    }
    return Matrix(l)
  } 

  setLagrangeMultipliers(lambda) {
    self.lambda = self._unflatten(lambda, self.lambda)
  }

  lagrangeMultipliers() {
    return self._flatten(self.lambda)
  }

  _zeroIneq(Matrix m, Matrix out) {
    for (c, k in m) {
      if (c>=0) out[k] = 0
    }
    return out 
  }

  _zeroIneq(u, out) {
    if (u>=0) return 0
    return out
  }

  _checkIneq(c, out) { // Checks whether entries in c are inside the feasible 
                       // region and sets the corresponding entry in out to zero
    var neq = self.adapter.countEqualityConstraints()
    for (k in neq...c.count()) {
      out[k]=self._zeroIneq(c[k], out[k])
    }
  }

  _inner(Matrix u, Matrix v) { return u.inner(v) }
  _inner(u, v) { return u*v }

  inner(c) { // Takes the inner product of the list c with the lagrange multipliers
    var total = 0 
    for (u,k in c) total+=self._inner(self.lambda[k], u)
    return total
  }

  value() { // Lagrangian function = f - lambda_i c_i
    var f = self.adapter.value() 
    var c = self.adapter.constraintValue()
    self._checkIneq(c, c)

    return f - self.inner(c)
  }

  varGradient() { // Gradient of lagrangian wrt original degrees of freedom
    var grad = self.adapter.gradient()
    var cgrad = self.adapter.constraintGradient()

    for (cg, k in cgrad) grad -= cg*self.lambda[k]

    return grad 
  }

  gradient() { // Gradient of Lagrangian is [ df - lambda_i dc_i , -c_i] 
    var c = self.adapter.constraintValue()
    self._checkIneq(c, c)
    var cv = self._flatten(c)

    return Matrix([[self.varGradient()],[-cv]]) 
  } 

  hessian() { // Hessian of Lagrangian is the kkt matrix 
    Error.throw("NOT IMPLEMENTED")
/*    var h = self.adapter.hessian()   
    if (!h) _OptNoHessErr.throw() 

    var chess = self.adapter.constraintHessian() 
    for (ch,k in chess) {
      if (!ch) _OptNoHessErr.throw("Constraint ${ch.clss()} does not provide a hessian.")
      h -= self.lambda[k]*ch
    }

    var cg = self.adapter.constraintGradient()
    var C = Matrix([cg])

    return Matrix([[h, -C],[-C.transpose(), 0]])*/
  }

  _checkConstraintValue(Matrix u, isEq, out) {
    for (c in u) {
      if (isEq || c<0) out.append(c)
    }
  }

  _checkConstraintValue(u, isEq, out) {
    if (isEq || u<0) out.append(u)
  }

  activeConstraintValue() {
    var cv = self.adapter.constraintValue()
    var neq = self.adapter.countEqualityConstraints()
    var l = []
    for (c, k in cv) {
      self._checkConstraintValue(c, k<neq, l)
    }
    return Matrix(l)
  }

  _checkConstraintGradient(Matrix u, grad, isEq, out) {
    if (isEq) {
      out.append(grad)
    } else {
      var l = []
      for (c, k in u) {
        if (isEq || c<0) l.append(grad.column(k))
      }
      out.append(Sparse([l]))
    }
  }

  _checkConstraintGradient(u, grad, isEq, out) {
    if (isEq || u<0) out.append(grad)
  }

  activeConstraintGradient() {
    var cv = self.adapter.constraintValue()
    var cgrad = self.adapter.constraintGradient()
    var neq = self.adapter.countEqualityConstraints()
    var l = []
    for (c, k in cv) {
      self._checkConstraintGradient(c, cgrad[k], k<neq, l)
    }
    return l 
  }

  activeUpdate(dl) { // Promote update onto active set
    var cv = self.adapter.constraintValue()
    var neq = self.adapter.countEqualityConstraints()
    var out = []
    var dli = 0 // Index into dl 
    for (c, k in cv) {
      if (ismatrix(c)) {
        for (u in c) {
          if (k<neq || u<0) {
            out.append(dl[dli])
            dli += 1 
          } else out.append(0)
        }
      } else {
        if (k<neq || c<0) {
          out.append(dl[dli])
          dli += 1 
        } else out.append(0)
      }
    }
    return Matrix(out)
  }

}

class SQPController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, penalty=1) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)

    if (!self.isConstrained()) _OptUnCons.throw()

    self.penalty = penalty // Initial penalty for linesearch
  }

  gradient() { // Ensure the gradient of the Lagrangian is reported
    return self.ladapter.gradient()
  }

  start() {
    self.ladapter = LagrangeMultiplierAdapter(self.adapter)
    self.lbfgs = LBFGSController(DeconstrainAdapter(self.adapter))
    self.lbfgs.start() 
  }

  begin() {
    self._ox = self.get() 
    self._ogradient = self.ladapter.varGradient() 
  }

  invHessian() {
    var x = self.adapter.get() 
    var n = x.count() 
    var h = []
    for (i in 0...n) {
      var xi = Matrix(n); xi[i]=1
      h.append(self.lbfgs._hmul(xi))
    }

    return Matrix([h]) 
  }

  kkt() { // Estimate kkt matrix explicitly
    var h = self.invHessian().inverse() // We have to reconstruct the inv. hessian and invert it

    var cg = self.adapter.constraintGradient()
    var C = Matrix([cg])
    var Ct = C.transpose()

    return Matrix([[h,-C],[-Ct,0]]) 
  }

  solve() {
    // Evaluate all elements of the KKT matrix
    var g = self.ladapter.varGradient() 

    var Hg = self.lbfgs._hmul(g) 

    var c = self.ladapter.activeConstraintValue()

    if (c.count()==0) { // Special case if the active set is empty
      return -Matrix([ [ Hg ],
                       [ 0*self.ladapter.lagrangeMultipliers() ] ])
    }

    var cg = self.ladapter.activeConstraintGradient()

    var C = Matrix([cg])
    var Ct = C.transpose()

    // Form the Schur complement
    var HC = self.lbfgs._hmul(C)
    var S = Ct*HC 

    var Sinvc = c/S
    var SinvCt = (Ct*Hg)/S

    var HCSinvCt = self.lbfgs._hmul(C*SinvCt)
    var HCCSc = self.lbfgs._hmul(C*Sinvc)

    var dl = self.ladapter.activeUpdate(- SinvCt + Sinvc)

    return -Matrix([ [Hg - HCSinvCt + HCCSc],
                     [dl] ])
  }

  _choosePenalty(dirn) {
    var maxpenalty=max(self.ladapter.lagrangeMultipliers())
    if (maxpenalty>self.penalty) self.penalty = 2*maxpenalty

    return self.penalty
  }

  linesearch(dirn) {
    var x0 = self.adapter.get()
    var ndof = x0.count() 
    var xdirn = dirn[0...ndof]

    var mu = self._choosePenalty(xdirn)

    var padapt = L1PenaltyAdapter(self.adapter, penalty=mu)
    var ls = DirectedLineSearchController(padapt, direction=xdirn)
    ls.step()

    self.adapter.set(x0)

    return ls.stepsize
  }

  step() {
    var x0 = self.ladapter.get()
    var ndof = self.adapter.get().count()

    var d = self.solve() // Find search direction
    var alpha = self.linesearch(d) 

    var dl = d.clone() // Evaluate gradient at new lagrange multipliers
    for (i in 0...ndof) dl[i]=0
    self.ladapter.set(x0 + alpha*dl)
    self._ogradient = self.ladapter.varGradient() 

    self.ladapter.set(x0 + alpha*d) // New set new value

    self.stepsize = alpha 
  }

  constraintNorm() {
    return PenaltyAdapter(self.adapter).constraintVector().norm(1)
  }

  reportStepsize() { 
    return "|constraints|=${self.constraintNorm()}" 
  }

  next() {
    // Note that we choose yk = grad L(x+alpha d, lambda k+1)-grad L(x, lambda k+1)
    // i.e. holding lambda constant
    var sk = self.get() - self._ox
    var yk = self.ladapter.varGradient() - self._ogradient

    self.lbfgs.update(sk, yk)
  }
}